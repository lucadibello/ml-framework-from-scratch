{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWP3_AEVeknK"
   },
   "source": [
    "# Assignment 3\n",
    "\n",
    "For submission, please choose \"file\"->\"Download\"->\"Download .py\", rename the file to \"firstname_lastname.py\" and upload it on the iCorsi platform. While we encourage you to discuss the assignments in groups, please write all the code yourself and do not share it with others! The deadline is Monday December 2th, 11:59 pm.\n",
    "\n",
    "For everything to work, all the cells in this notebook have to run in the correct order. When in doubt, just choose \"Runtime\"->\"Run all\" and it will run all cells from the start until an exception is encountered.\n",
    "\n",
    "For comments and questions, contact me: vincent.herrmann@usi.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx2VI17DfiB5"
   },
   "source": [
    "\n",
    "# Simple Machine Learning Framework\n",
    "(30 points)\n",
    "\n",
    "In this part we are going to implement a very simple machine learning framework. The goal is that at the end of this part, you will have a rough idea what is happening inside a real world ML framework (e.g. PyTorch, TensorFlow, JAX, etc). Using this framework you will be able to easily build and train arbitrary fully connected neural networks. It will have the following building blocks: linear layer, sequential layer, the tanh activation function, and the softmax + cross-entropy loss. The skeleton is given, you will have to fill in the missing parts. Pay attention to the description in the skeleton file: it clarifies the task further, and also gives some useful hints. **You are not allowed to change the signature of the functions (name, the format of the arguments or output)**. They will be auto-checked, so their interface should match.\n",
    "\n",
    "You will test your framework on a synthetic two-class spiral dataset. You will use a simple, 3 layer network with tanh activation function on hidden layers and a softmax output layer. The output sizes are 50, 30, 2, respectively.\n",
    "\n",
    "- (5 points) Implement the ReLU activation function and its derivative in the code skeleton.\n",
    "\n",
    "- (5 points) Implement forward and backward pass for the linear layer.\n",
    "\n",
    "- (10 points) Implement forward and backward pass for the cross-entropy loss.\n",
    "\n",
    "- (10 points) Create the network and implement a single step of training with weight decay.\n",
    "\n",
    "Your task is to fill missing parts of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_xdPwI26wj4"
   },
   "source": [
    "### Detailed instructions\n",
    "You will have to implement a very primitive machine learning framework. The skeleton is given. You have to fill the\n",
    "blocks starting with `## Implement` and ending with `## End`. The goal is to provide you an insight into how machine learning frameworks work\n",
    "while practicing the most important details from the class.\n",
    "\n",
    "You have to implement the following modules (or call them layers?):\n",
    "- The ReLU activation function\n",
    "- A learnable linear layer\n",
    "- The Softmax+Cross-Entropy loss\n",
    "\n",
    "The linear layer is also called perceptron, fully connected layer, etc. The bias term is not included in the\n",
    "network weight matrix W because of performance reasons (concatenating the 1 to the input is slow). This is the case for the real-world ML frameworks too.\n",
    "\n",
    "All the functions you have to implement has their signature pre-specified, with annotated types. You should _not_ change this, because it will be tested by an automated system which depends on the current interface.\n",
    "\n",
    "The sequential layer is already implemented. It receives a list of layers for the constructor argument, and calls them in order on forward and in reverse order on backward. It is just syntactic sugar that makes the code much nicer. For example, you can\n",
    "create a two-layer neural network with ReLU activation functions with `net = Sequential([Linear(5, 8), ReLU(),\n",
    "Linear(8, 3), ReLU()])` and then run a forward pass using `output = net.forward(your data)`.\n",
    "\n",
    "All the modules have a `forward()` and a `backward()` function. `forward()` receives one argument (except for the loss which takes two) and\n",
    "returns that layer's output. The `backward()` receives the dL/dout (i.e., the gradient of the loss with respect to the output of the last `forward()` call), flowing back on the output of the layer, and should return a `BackwardResult` object with the following fields:\n",
    "- `variable_grads`: a dict of gradients, where the keys are the same as in the keys in the layer's .var. The values are numpy arrays representing the gradient for that variable.\n",
    "- `input_grads`: a numpy array representing the gradient for the input (that was passed to the layer in the forward pass).\n",
    "\n",
    "The `backward()` does not receive the forward pass's input, although it might be needed for the gradient calculation. You should save them in the forward pass for later use in the backward pass. You don't have to worry about most of this, as it is already implemented in the skeleton. There are 2 important takeaways: you have to calculate the gradient of both of your variables and the layer input in the backward pass, and if you need to reuse the input from the forward pass, you need to save it.\n",
    "\n",
    "You will also have to implement the function `train_one_step()`, which does one step of weight update based on the training data and the learning rate. It should run backpropagation, followed by gradient descent. It should also include weight decay, meaning that weights and biases are being punished for being too large. This equivalent to having an additional loss term $L_{wd} = \\beta \\sum_l \\big(|w_l|^2 + |b|^2 \\big)$, where $\\beta$ is the weight decay factor and $l$ is the index of the layer. It is easiest to implement it directly in the `train_one_step()` function. A good value for $\\beta$ is $0.005$.\n",
    "\n",
    "Automatic gradient checking is already implemented. It works by comparing your implementation of the analytic gradient with a numerical approximation. This is done by iterating over all the elements of all variables, nudging them a bit in both directions, and recalculating the network output. Based on that, we can calculate what the gradient should be if we assume that the forward pass is correct. The method is known as numerical differentiation, specifically the symmetric difference quotient. If the gradient checking passes and the error is around 0.008, your implementation of the backward passes is probably correct.\n",
    "\n",
    "Finally, you would have to complete the `create_network()` function, which should return a Sequential neural network of 3 layers: a ReLU input layer with 2 inputs and 50 outputs, a ReLU hidden layer with 30 outputs, and finally an output layer with 2 outputs. Our loss function is the Softmax+Cross-Entropy loss. Usually for two-way classification we don't use softmax, but we will need it for the MNIST part anyway, so we use it here as well.\n",
    "\n",
    "At the end of the training, your loss should be below 0.04. Don't worry if it differs a bit, but a significantly higher value may indicate a problem.\n",
    "\n",
    "There are asserts at many points in the code that will check the shapes of the gradients. Remember: the gradient for a variable must have the same shape as the variable itself. Imagine the variables and the network inputs/outputs as a cable with a given number of wires: no matter in which direction you pass the data, the number of wires is the same.\n",
    "\n",
    "Please do your calculations in a vectorized way. Otherwise, it will be painfully slow. This also means your data has a batch dimension. The input to each module is not a data vector $x$, but a $n \\times d$ matrix, where $n$ is the batch size and $d$ the dimensionality of the data vector. The only places where you might need a for-loop are the `Sequential` layer and the `train_one_step` function.\n",
    "\n",
    "Good luck, I hope you'll enjoy it :)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "7ONSb1q-Or9W"
   },
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Nothing to do in this cell.\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from urllib import request\n",
    "import gzip\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Interface definitions\n",
    "class Layer:\n",
    "    var: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    @dataclass\n",
    "    class BackwardResult:\n",
    "        variable_grads: Dict[str, np.ndarray]\n",
    "        input_grads: np.ndarray\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, error: np.ndarray) -> BackwardResult:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuDxCwC-9pmL"
   },
   "source": [
    "### ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "h-VxMSJa957M"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        result = np.maximum(0, x)\n",
    "        self.saved_variables = {\"result\": result}\n",
    "        return result\n",
    "\n",
    "    def backward(self, grad_in: np.ndarray) -> Layer.BackwardResult:\n",
    "        # result of forward pass, needed to compute the derivative of ReLu(X) -> ReLu'(X)\n",
    "        relu_x = self.saved_variables[\"result\"]\n",
    "        # Derviative of ReLU: f'(x) = 1 if x > 0 else 0\n",
    "        d_x = np.where(relu_x > 0, 1, 0) * grad_in\n",
    "\n",
    "        # ensure dimensions match\n",
    "        assert d_x.shape == relu_x.shape, \"Input: grad shape differs: %s %s\" % (d_x.shape, relu_x.shape)\n",
    "        \n",
    "        # Clean up internal state + return backward result\n",
    "        self.saved_variables = None\n",
    "        return Layer.BackwardResult({}, d_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "V8-THkHV-A4L"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        self.var = {\n",
    "            \"W\": np.random.normal(0, np.sqrt(2 / (input_size + output_size)), (input_size, output_size)),\n",
    "            \"b\": np.zeros((output_size), dtype=np.float32)\n",
    "        }\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        W = self.var['W'] # weights\n",
    "        b = self.var['b'] # bias\n",
    "\n",
    "        # Compute the linear transformation of x\n",
    "        y =  np.dot(x, W) + b\n",
    "\n",
    "        # Save the input for the backward pass\n",
    "        self.saved_variables = {\n",
    "            \"input\": x,\n",
    "        }\n",
    "\n",
    "        # return the result\n",
    "        return y\n",
    "\n",
    "    def backward(self, grad_in: np.ndarray) -> Layer.BackwardResult:\n",
    "        W = self.var['W'] # weights\n",
    "        x = self.saved_variables[\"input\"]\n",
    "\n",
    "        ## Implement\n",
    "\n",
    "        # Gradient of the loss function L with respect to the weights W\n",
    "        # --- Math resoning ---\n",
    "        # dL/dW = dL/dy * dy/dW\n",
    "        # where:\n",
    "        #   - dy/dW = x (input transposed for alignment)\n",
    "        #   - dL/dy = grad_in (gradient from the next layer)\n",
    "        # For a batch of inputs, the gradient is computed as:\n",
    "        # dL/dW = x^T * grad_in\n",
    "        dW = x.T @ grad_in\n",
    "\n",
    "        # Gradient of the loss function L with respect to the bias b\n",
    "        # --- Math resoning ---\n",
    "        # dL/db = dL/dy * dy/db\n",
    "        # where:\n",
    "        #   - dL/dy = grad_in (gradient from the next layer)\n",
    "        #   - dy/db = 1 (bias only shifts the output, so its derivative is 1)\n",
    "        # Therefore, for each element of the bias vector b:\n",
    "        #   dL/db[j] = sum of all gradients dL/dy[i, j] over the batch dimension i\n",
    "        # For a batch of inputs, we compute the sum along axis 0 to aggregate gradients for each bias term:\n",
    "        db = grad_in.sum(axis=0)\n",
    "\n",
    "        # Gradient of the loss function L with respect to the input x\n",
    "        # --- Math resoning ---\n",
    "        # dL/dx = dL/dy * dy/dx\n",
    "        # where:\n",
    "        #   - dL/dy = grad_in (gradient from the next layer)\n",
    "        #   - dy/dx = W (weights) \n",
    "        d_inputs = grad_in @ W.T\n",
    "\n",
    "        # ensure dimensions match\n",
    "        assert d_inputs.shape == x.shape, \"Input: grad shape differs: %s %s\" % (d_inputs.shape, x.shape)\n",
    "        assert dW.shape == self.var[\"W\"].shape, \"W: grad shape differs: %s %s\" % (dW.shape, self.var[\"W\"].shape)\n",
    "        assert db.shape == self.var[\"b\"].shape, \"b: grad shape differs: %s %s\" % (db.shape, self.var[\"b\"].shape)\n",
    "\n",
    "        self.saved_variables = None\n",
    "        updates = {\"W\": dW,\n",
    "                   \"b\": db}\n",
    "        return Layer.BackwardResult(updates, d_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVfmF0qI-ElG"
   },
   "source": [
    "### Sequential Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "1WT0VULy-GkN"
   },
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Nothing to do in this cell.\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "class Sequential(Layer):\n",
    "    class RefDict(dict):\n",
    "        def __setitem__(self, k, v):\n",
    "            assert k in self, \"Trying to set a non-existing variable %s\" % k\n",
    "            ref = super().__getitem__(k)\n",
    "            ref[0][ref[1]] = v\n",
    "\n",
    "        def __getitem__(self, k):\n",
    "            ref = super().__getitem__(k)\n",
    "            return ref[0][ref[1]]\n",
    "\n",
    "        def items(self) -> Tuple[str, np.ndarray]:\n",
    "            for k in self.keys():\n",
    "                yield k, self[k]\n",
    "\n",
    "    def __init__(self, list_of_modules: List[Layer]):\n",
    "        self.modules = list_of_modules\n",
    "\n",
    "        refs = {}\n",
    "        for i, m in enumerate(self.modules):\n",
    "            print(f\"mod_{i}: {m}\")\n",
    "            refs.update({\"mod_%d.%s\" % (i,k): (m.var, k) for k in m.var.keys()})\n",
    "\n",
    "        self.var = self.RefDict(refs)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        x = input\n",
    "        for i, m in enumerate(self.modules):\n",
    "            x = m.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_in: np.ndarray) -> Layer.BackwardResult:\n",
    "        variable_grads = {}\n",
    "\n",
    "        for module_index in reversed(range(len(self.modules))):\n",
    "            module = self.modules[module_index]\n",
    "\n",
    "            grads = module.backward(grad_in)\n",
    "\n",
    "            grad_in = grads.input_grads\n",
    "            variable_grads.update({\"mod_%d.%s\" % (module_index, k): v for k, v in grads.variable_grads.items()})\n",
    "\n",
    "        return Layer.BackwardResult(variable_grads, grad_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "085VuAbd-KSN"
   },
   "source": [
    "### Softmax + Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7E1xdc16-NUU"
   },
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def forward(self, inputs: np.ndarray, targets: np.ndarray) -> float:\n",
    "        y = inputs\n",
    "        t = targets\n",
    "        n = inputs.shape[0]\n",
    "        num_classes = y.shape[1]\n",
    "\n",
    "        ## Remember that the here targets are not one-hot vectors, but integers specifying the class.\n",
    "        ## The loss function has to return a single scalar, so we have to take the mean over the batch dimension.\n",
    "        ## Don't forget to save your variables needed for backward to self.saved_variables.\n",
    "\n",
    "        # Convert the targets to one-hot vectors (targer = index of the class to set to 1)\n",
    "        one_hot_t = np.zeros((n, num_classes)) # same shape as y\n",
    "        one_hot_t[np.arange(n), t] = 1 # set the correct class to 1\n",
    "\n",
    "        # Numerically stable softmax computation\n",
    "        # Source: https://www.parasdahal.com/softmax-crossentropy\n",
    "\n",
    "        # Compute the softmax\n",
    "        # S = exp(y) / sum(exp(y))\n",
    "        shifted_logits = y - np.max(y, axis=1, keepdims=True)\n",
    "        exp_shifted = np.exp(shifted_logits)\n",
    "        softmax = exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        # H(y, softmax) = - sum(t * log(softmax))\n",
    "        mean_crossentropy = -np.sum(one_hot_t * np.log(softmax)) / n\n",
    "\n",
    "        self.saved_variables = {\n",
    "            \"S\": softmax,\n",
    "            \"t\": one_hot_t,\n",
    "            \"n\": n\n",
    "        }\n",
    "\n",
    "        return mean_crossentropy\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        softmax = self.saved_variables[\"S\"]\n",
    "        one_hot_t = self.saved_variables[\"t\"]\n",
    "        n = self.saved_variables[\"n\"]\n",
    "\n",
    "        ## Implement\n",
    "\n",
    "        # To compute the gradient of the cross-entropy loss with respect to the input of the softmax layer,\n",
    "        # we can use the following formula:\n",
    "        # dL/dy = softmax - t\n",
    "\n",
    "        d_inputs = (softmax - one_hot_t) / n\n",
    "\n",
    "        ## End\n",
    "\n",
    "        assert d_inputs.shape == softmax.shape, f\"Error shape doesn't match prediction: {d_inputs.shape} {softmax.shape}\"\n",
    "        self.saved_variables = None\n",
    "        return d_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtiWzHNF-gzH"
   },
   "source": [
    "### Train One Step & Create Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OaFX6dmPVsC"
   },
   "outputs": [],
   "source": [
    "def train_one_step(model: Layer, loss: Loss, learning_rate: float, input: np.ndarray, target: np.ndarray, weight_decay=0.0005) -> float:\n",
    "\n",
    "    ## Implement\n",
    "    ## Calculate the loss value for the given inputs and targets, calculate the\n",
    "    ## gradients via backpropagation, and update all the weights and biases via\n",
    "    ## gradient descent (use a for-loop to go through all the learnable\n",
    "    ## parameters of the model)\n",
    "\n",
    "    # Forward pass\n",
    "    # loss_value = SoftmaxCrossEntropy().forward(model.forward(input), target)\n",
    "    pred = model.forward(input)\n",
    "\n",
    "    # Compute loss value\n",
    "    loss_value = loss.forward(pred, target)\n",
    "\n",
    "    # Compute the loss gradients\n",
    "    loss_gradients = loss.backward()\n",
    "    # Compute the gradients of the model\n",
    "    variable_gradients = model.backward(loss_gradients)\n",
    "\n",
    "    # Update the weights and biases of the model using the computed gradients\n",
    "    # TODO: to complete\n",
    "    for key, grad in variable_gradients.items():\n",
    "        print(\"key\", key, \"grad\", grad)\n",
    "\n",
    "    ## End\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "def create_network() -> Layer:\n",
    "\n",
    "    ## Implement\n",
    "\n",
    "    network = Sequential([\n",
    "        Linear(2, 20), ReLU(),    # Input layer (2D spiral data) with a larger hidden layer\n",
    "        Linear(20, 10), ReLU(),   # Second hidden layer\n",
    "        Linear(10, 5), ReLU(),    # Third hidden layer\n",
    "        Linear(5, 2),             # Output layer for classification\n",
    "    ])\n",
    "\n",
    "    ## End\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PskJzWLq-oGY"
   },
   "source": [
    "### Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "jAuj5ZKV-rej"
   },
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "# Nothing to do in this cell.\n",
    "###############################################################################################################\n",
    "\n",
    "def gradient_check():\n",
    "    X, T = twospirals(n_points=10)\n",
    "    print(X.shape, T.shape)\n",
    "    NN = create_network()\n",
    "    eps = 0.0001\n",
    "\n",
    "    loss = SoftmaxCrossEntropy()\n",
    "    loss.forward(NN.forward(X), T)\n",
    "    variable_gradients = NN.backward(loss.backward()).variable_grads\n",
    "\n",
    "    all_succeeded = True\n",
    "\n",
    "    # Check all variables. Variables will be flattened (reshape(-1)), in order to be able to generate a single index.\n",
    "    for key, value in NN.var.items():\n",
    "        variable = NN.var[key].reshape(-1)\n",
    "        variable_gradient = variable_gradients[key].reshape(-1)\n",
    "        success = True\n",
    "\n",
    "        if NN.var[key].shape != variable_gradients[key].shape:\n",
    "            print(\"[FAIL]: %s: Shape differs: %s %s\" % (key, NN.var[key].shape, variable_gradients[key].shape))\n",
    "            success = False\n",
    "            break\n",
    "\n",
    "        # Check all elements in the variable\n",
    "        for index in range(variable.shape[0]):\n",
    "            var_backup = variable[index]\n",
    "\n",
    "            analytic_grad = variable_gradient[index]\n",
    "\n",
    "            variable[index] = var_backup + eps\n",
    "            perturbed_loss_plus = loss.forward(NN.forward(X), T)\n",
    "            variable[index] = var_backup - eps\n",
    "            perturbed_loss_minus = loss.forward(NN.forward(X), T)\n",
    "            numeric_grad = (perturbed_loss_plus - perturbed_loss_minus) / (2*eps)\n",
    "\n",
    "            variable[index] = var_backup\n",
    "            if abs(numeric_grad - analytic_grad) > 0.00001:\n",
    "                print(\"[FAIL]: %s: Grad differs: numerical: %f, analytical %f\" % (key, numeric_grad, analytic_grad))\n",
    "                success = False\n",
    "                break\n",
    "\n",
    "        if success:\n",
    "            print(\"[OK]: %s\" % key)\n",
    "\n",
    "        all_succeeded = all_succeeded and success\n",
    "\n",
    "    return all_succeeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIizvQ5B-Y8Z"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "XWdnpB8DPZvK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the network\n",
      "(20, 2) (20,)\n",
      "mod_0: <__main__.Linear object at 0x113835c50>\n",
      "mod_1: <__main__.ReLU object at 0x11377de10>\n",
      "mod_2: <__main__.Linear object at 0x11134bfd0>\n",
      "mod_3: <__main__.ReLU object at 0x1137d74d0>\n",
      "mod_4: <__main__.Linear object at 0x113834bd0>\n",
      "mod_5: <__main__.ReLU object at 0x113836a10>\n",
      "mod_6: <__main__.Linear object at 0x113835950>\n",
      "[FAIL]: mod_0.W: Grad differs: numerical: 0.042813, analytical 0.045993\n",
      "[OK]: mod_0.b\n",
      "[OK]: mod_2.W\n",
      "[OK]: mod_2.b\n",
      "[OK]: mod_4.W\n",
      "[OK]: mod_4.b\n",
      "[OK]: mod_6.W\n",
      "[OK]: mod_6.b\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed. Not training, because your gradients are not good.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking the network\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gradient_check():\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed. Not training, because your gradients are not good.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m X, T \u001b[38;5;241m=\u001b[39m twospirals(n_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.4\u001b[39m, twist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Failed. Not training, because your gradients are not good."
     ]
    }
   ],
   "source": [
    "###############################################################################################################\n",
    "# Nothing to do in this cell. Run it to train your model.\n",
    "###############################################################################################################\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "\n",
    "def twospirals(n_points=120, noise=1.4, twist=600):\n",
    "    \"\"\"\n",
    "      Returns a two spirals dataset.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    n = np.sqrt(np.random.rand(n_points, 1)) * twist * (2 * np.pi) / 360\n",
    "    d1x = -np.cos(n) * n + np.random.rand(n_points, 1) * noise\n",
    "    d1y = np.sin(n) * n + np.random.rand(n_points, 1) * noise\n",
    "    X, T = (np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y)))),\n",
    "            np.hstack((np.zeros(n_points).astype(int), np.ones(n_points).astype(int))))\n",
    "    return X, T\n",
    "\n",
    "\n",
    "def plot_data(X, T):\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=40, c=T, cmap=plt.cm.Spectral)\n",
    "\n",
    "\n",
    "def plot_boundary(model, X, targets, threshold=0.0):\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    y = model.forward(X_grid)[:, 0]\n",
    "\n",
    "    ax.contourf(xx, yy, y.reshape(*xx.shape) < threshold, alpha=0.5)\n",
    "    plot_data(X, targets)\n",
    "    ax.set_ylim([y_min, y_max])\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Checking the network\")\n",
    "if not gradient_check():\n",
    "    raise Exception(\"Failed. Not training, because your gradients are not good.\")\n",
    "print(\"Done. Training...\")\n",
    "\n",
    "X, T = twospirals(n_points=200, noise=1.4, twist=600)\n",
    "NN = create_network()\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "learning_rate = 0.02\n",
    "\n",
    "for i in range(20000):\n",
    "    curr_error = train_one_step(NN, loss, learning_rate, X, T, weight_decay=0.0005)\n",
    "    if i % 200 == 0:\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        plot_boundary(NN, X, T, 0.5)\n",
    "        print(\"step: \", i, \" cost: \", curr_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8-wIODv6Uko"
   },
   "source": [
    "# Handwritten Digit Recognition\n",
    "\n",
    "(20 points)\n",
    "\n",
    "In this part you will use your ML framework to train a network that can classify handwritten digits. The network is a very simple 2 layer network, with layer output sizes of 20 and 10 respectively. Again, the skeleton is given, and you have to fill in the missing parts. After this part, you will have a rough idea how to build a proper training pipeline. Pay attention to the description in the skeleton file: it clarifies the your task further, and also gives some useful hints. Reuse as much as you can from framework.py by importing the corresponding functions and classes (e.g the layers, the `train_one_step()` function, etc).\n",
    "\n",
    "- (3 points) Split the data into a training and a validation set.  \n",
    "- (8 points) Implement one function for testing the network on the validation set and one for testing it on the test set.\n",
    "- (6 points) Implement the main training loop.\n",
    "- (3 points) Implement early stopping.\n",
    "\n",
    "Your task is to fill missing parts of the minst.py file. Run it with Python 3 in order to verify whether your solution is correct. Note: The mnist.py and framework.py must be in the same folder for this to work. You will find lots of hints in the templates. Note: in case you return wrongly shaped vectors, the code might fail outside the part you had to fill, but this does not mean that your part is right and the template is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRMAcOGo2VnG"
   },
   "source": [
    "### Detailed Instructions\n",
    "In this assignment, you will learn how to train a model with early stopping with the simple machine learning\n",
    "framework's help from the previous assignment. The intent is to show you how to train a machine learning model.\n",
    "It is very similar to what you would do in a real ML framework.\n",
    "\n",
    "We provide some code to download and load MNIST digits. The MNIST comes with a separate test and training set, but\n",
    "not a validation set. Your task is to split the official \"train\" set to train and validation set and train the\n",
    "network with early stopping.\n",
    "\n",
    "Some notes:\n",
    " * an EPOCH is a pass through the whole training set once.\n",
    " * validation accuracy is the percentage of correctly classified digits on the validation set.\n",
    " * early stopping is when you avoid overfitting the model by measuring validation accuracy every N steps (e.g.,\n",
    "   every epoch) and stopping your training when your model begins to worsen on the validation set. You can do that by\n",
    "   keeping track of the best validation accuracy so far and its epoch, and stopping if the validation\n",
    "   accuracy has not improved in the last M steps (e.g., last 10 epochs).\n",
    "   (A better way to do this is to keep the weights of the best performing model, but that is harder since you need a\n",
    "   way to save and reload weights of the model. We keep it simple instead and use the last, slightly worse model).\n",
    " * test accuracy is the percentage of correctly classified digits on the test set.\n",
    " * watch out: if you load batch_size of data by NumPy slicing, it is not guaranteed that you will actually get\n",
    "   batch_size of them: if your array length is not divisable by batch_size, you will get the remainder as the last\n",
    "   batch. Take that in account when calculating the percentages: use shape[0] to determine the real number of\n",
    "   elements in the current batch of data.\n",
    " * verify() should be used both in the validate() and test() functions for error measurement\n",
    "   without code duplication (just the input data should be different).\n",
    " * this is a 10-way classification task, so your network will have 10 outputs, one for each digit. It\n",
    "   can be trained by Softmax nonlinearity followed by a Cross-Entropy loss function. So for every image, you get 10\n",
    "   outputs. To figure out which one is the correct class, you should find which is the most active.\n",
    " * MNIST comes with black-and-white binary images, with a background of 0 and foreground of 255. Each image is 28x28\n",
    "   matrix. To feed that to the model, we flatten it to a 784 (28*28) length vector and normalize it by 255, so the\n",
    "   backround becomes 0 and the foreground 1.0. Labels are integers between 0-9. You don't have to worry about this;\n",
    "   it's already done for you. The networks usually like to receive an input in range -1 .. 1 or generally the mean\n",
    "   near 0, and the standard deviation near 1 (as the majority of MNIST pixels is black, normalizing it to 0..1 range\n",
    "   is good enough).\n",
    "\n",
    "Your final test accuracy should be close to 96%.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "wYXDxG5R2g3I",
    "outputId": "58ee45a9-f381-404c-bec6-8287f2aaf5b6"
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Nothing to do in this cell.\n",
    "#######################################################################################################################\n",
    "\n",
    "class MNIST:\n",
    "    URL = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "\n",
    "    def __init__(self, set, cache=\"./cache\"):\n",
    "        os.makedirs(cache, exist_ok=True)\n",
    "\n",
    "        path = os.path.join(cache, \"mnist.npz\")\n",
    "        if not os.path.isfile(path):\n",
    "            request.urlretrieve(self.URL, path)\n",
    "\n",
    "        data = np.load(path)\n",
    "\n",
    "        assert set in [\"train\", \"test\"], \"set must be either train or test\"\n",
    "\n",
    "        self.images = data[\"x_\" + set].astype(float) / 255.0\n",
    "        self.labels = data[\"y_\" + set]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.images.shape[0]\n",
    "\n",
    "\n",
    "train_validation_set = MNIST(\"train\")\n",
    "test_set = MNIST(\"test\")\n",
    "\n",
    "n_train = int(0.7 * len(train_validation_set))\n",
    "print(\"MNIST:\")\n",
    "print(\"   Train set size:\", n_train)\n",
    "print(\"   Validation set size:\", len(train_validation_set) - n_train)\n",
    "print(\"   Test set size\", len(test_set))\n",
    "\n",
    "np.random.seed(1234)\n",
    "batch_size = 64\n",
    "\n",
    "loss = SoftmaxCrossEntropy()\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(28*28, 20),\n",
    "    ReLU(),\n",
    "    Linear(20, 10),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtcBuGog_TM5"
   },
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vcr1DDu73tmy"
   },
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(train_validation_set))\n",
    "\n",
    "## Implement\n",
    "## Hint: you should split indices to 2 parts: a training and a validation one. Later when loading a batch of data,\n",
    "## just iterate over those indices by loading \"batch_size\" of them at once, and load data from the dataset by\n",
    "## train_validation_set.images[your_indices[i: i+batch_size]] and\n",
    "## train_validation_set.labels[your_indices[i: i+batch_size]]\n",
    "\n",
    "# train_indices =\n",
    "# validation_indices =\n",
    "\n",
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D5Izz4UBhTp"
   },
   "source": [
    "### Verify, Test and Validate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAA6DEzz6Rnq"
   },
   "outputs": [],
   "source": [
    "def verify(images: np.ndarray, targets: np.ndarray) -> Tuple[int, int]:\n",
    "    ## Implement\n",
    "\n",
    "    # num_ok =\n",
    "    # total_num =\n",
    "\n",
    "    ## End\n",
    "    return num_ok, total_num\n",
    "\n",
    "\n",
    "def test() -> float:\n",
    "    accumulated_ok = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(0, len(test_set), batch_size):\n",
    "        images = test_set.images[i:i + batch_size]\n",
    "        labels = test_set.labels[i:i + batch_size]\n",
    "\n",
    "        ## Implement. Use the verify() function to verify your data.\n",
    "\n",
    "        ## End\n",
    "\n",
    "    return accumulated_ok / count * 100.0\n",
    "\n",
    "\n",
    "def validate() -> float:\n",
    "    accumulated_ok = 0.0\n",
    "    count = 0\n",
    "\n",
    "    ## Implement. Use the verify() function to verify your data.\n",
    "\n",
    "    ## End\n",
    "\n",
    "    return accumulated_ok/count * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFtbUNCaBobU"
   },
   "source": [
    "### Train Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUTnGmWS3GRt"
   },
   "outputs": [],
   "source": [
    "## You should update these: best_validation_accuracy is the best validation set accuracy so far, best_epoch is the\n",
    "## epoch of this best validation accuracy (the later can be initialized by anything, as the accuracy will be for sure\n",
    "## better than 0, so it will be updated for sure).\n",
    "best_validation_accuracy = 0\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(200):\n",
    "    ## Implement\n",
    "    ## Use a for-loop to go through all the mini-batches (with the given batch\n",
    "    ## size) in the training set, then compute the validation accuracy using\n",
    "    ## validate()\n",
    "\n",
    "    # current_loss_value =\n",
    "\n",
    "    # validation_accuracy =\n",
    "\n",
    "    ## End\n",
    "\n",
    "    print(\"Epoch %d: loss: %f, validation accuracy: %.2f%%\" % (epoch, current_loss_value, validation_accuracy))\n",
    "\n",
    "    ## Implement\n",
    "    ## Hint: you should check if current accuracy is better that the best so far. If it is not, check before how many\n",
    "    ## iterations ago the best one came, and terminate if it is more than 10. Also update the best_* variables\n",
    "    ## if needed.\n",
    "\n",
    "    # End\n",
    "\n",
    "print(\"Test set performance: %.2f%%\" % test())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
