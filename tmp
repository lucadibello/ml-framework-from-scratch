# Gradient of the loss function L with respect to the weights W
# --- Math resoning ---
# dL/dW = dL/dy * dy/dW
# where:
#   - dy/dW = x (input transposed for alignment)
#   - dL/dy = grad_in (gradient from the next layer)
# For a batch of inputs, the gradient is computed as:
# dL/dW = x^T * grad_in
dW = x.T @ grad_in

# Gradient of the loss function L with respect to the bias b
# --- Math resoning ---
# dL/db = dL/dy * dy/db
# where:
#   - dL/dy = grad_in (gradient from the next layer)
#   - dy/db = 1 (bias only shifts the output, so its derivative is 1)
# Therefore, for each element of the bias vector b:
#   dL/db[j] = sum of all gradients dL/dy[i, j] over the batch dimension i
# For a batch of inputs, we compute the sum along axis 0 to aggregate gradients for each bias term:
db = grad_in.sum(axis=0)

# Gradient of the loss function L with respect to the input x
# --- Math resoning ---
# dL/dx = dL/dy * dy/dx
# where:
#   - dL/dy = grad_in (gradient from the next layer)
#   - dy/dx = W (weights) 
d_inputs = grad_in @ W.T