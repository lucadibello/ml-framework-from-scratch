{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Handwritten Digit Recognition\n",
    "\n",
    "This example shows how to use the machine learning framework to train a model for handwritten digit recognition. For this task, we will use the MNIST dataset, which is a dataset of 28x28 grayscale images of handwritten digits (0-9). The dataset contains 42,000 training images and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import required framework layers\n",
    "from framework.layers import Sequential, Linear\n",
    "from framework.activations import ReLU\n",
    "from framework.loss import SoftmaxCrossEntropy\n",
    "from framework.network import train_one_step\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_0: <framework.layers.linear.Linear object at 0x12f761e80>\n",
      "mod_1: <framework.activations.relu.ReLU object at 0x12f760680>\n",
      "mod_2: <framework.layers.linear.Linear object at 0x12f7382c0>\n"
     ]
    }
   ],
   "source": [
    "# Create simple network\n",
    "model = Sequential([\n",
    "    Linear(28*28, 20), # 28*28 input features (flat images), 20 output features\n",
    "    ReLU(), # Activation function\n",
    "    Linear(20, 10), # 20 input features, 10 output features\n",
    "])\n",
    "\n",
    "# Define loss function\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# Learning rate: step size for the optimizer\n",
    "learning_rate = 0.1\n",
    "# Batch size: number of samples per batch\n",
    "batch_size = 64\n",
    "# Number of epochs: number of times the dataset will be passed through the network\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST:\n",
      "   Train set size: 42000\n",
      "   Validation set size: 18000\n",
      "   Test set size 10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MNIST:\n",
    "    URL = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "\n",
    "    def __init__(self, set, cache=\"./cache\"):\n",
    "        os.makedirs(cache, exist_ok=True)\n",
    "\n",
    "        path = os.path.join(cache, \"mnist.npz\")\n",
    "        if not os.path.isfile(path):\n",
    "            request.urlretrieve(self.URL, path)\n",
    "\n",
    "        data = np.load(path)\n",
    "\n",
    "        assert set in [\"train\", \"test\"], \"set must be either train or test\"\n",
    "\n",
    "        self.images = data[\"x_\" + set].astype(float) / 255.0\n",
    "        self.labels = data[\"y_\" + set]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.images.shape[0]\n",
    "\n",
    "\n",
    "train_validation_set = MNIST(\"train\")\n",
    "test_set = MNIST(\"test\")\n",
    "\n",
    "n_train = int(0.7 * len(train_validation_set))\n",
    "print(\"MNIST:\")\n",
    "print(\"   Train set size:\", n_train)\n",
    "print(\"   Validation set size:\", len(train_validation_set) - n_train)\n",
    "print(\"   Test set size\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(train_validation_set))\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_indices = indices[:n_train]\n",
    "validation_indices = indices[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliy functions for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def verify(images: np.ndarray, targets: np.ndarray) -> Tuple[int, int]:\n",
    "    # Forward pass\n",
    "    pred = model.forward(images)\n",
    "\n",
    "    # Comupute number of correct predictions\n",
    "    # NOTE: The maximum value in the probability vector is the class chosen by the model\n",
    "    #       argmax --> return the index of the maximum value == predicted class by the model\n",
    "    num_ok = np.sum(np.argmax(pred, axis=1) == targets)\n",
    "    # Total number of images\n",
    "    total_num = targets.shape[0]\n",
    "    \n",
    "    return num_ok, total_num\n",
    "\n",
    "\n",
    "def test() -> float:\n",
    "    accumulated_ok = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # use test set\n",
    "    for i in range(0, len(test_set), batch_size):\n",
    "        images = test_set.images[i:i + batch_size].reshape(-1, 28*28)\n",
    "        labels = test_set.labels[i:i + batch_size]\n",
    "\n",
    "        # Verify the data\n",
    "        num_ok, total_num = verify(images, labels)\n",
    "        accumulated_ok += num_ok\n",
    "        count += total_num\n",
    "\n",
    "    return accumulated_ok / count * 100.0\n",
    "\n",
    "\n",
    "def validate() -> float:\n",
    "    accumulated_ok = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # use validation set\n",
    "    for i in range(0, len(train_validation_set), batch_size):\n",
    "        images = train_validation_set.images[i:i + batch_size].reshape(-1, 28*28)\n",
    "        labels = train_validation_set.labels[i:i + batch_size]\n",
    "\n",
    "        # Verify the data\n",
    "        num_ok, total_num = verify(images, labels)\n",
    "        accumulated_ok += num_ok\n",
    "        count += total_num\n",
    "\n",
    "    return accumulated_ok/count * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with early stopping\n",
    "\n",
    "Rather than training the model for a fixed number of epochs, we will use early stopping to stop training when the validation loss stops decreasing. This helps prevent overfitting and can improve the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Shape of the training set:  (42000, 28, 28)\n",
      "Shape of the labels:  (42000,)\n",
      "Epoch 0: loss: 0.178793, validation accuracy: 91.27%\n",
      "Epoch 1: loss: 0.089356, validation accuracy: 92.77%\n",
      "Epoch 2: loss: 0.063351, validation accuracy: 93.55%\n",
      "Epoch 3: loss: 0.052029, validation accuracy: 94.08%\n",
      "Epoch 4: loss: 0.045468, validation accuracy: 94.47%\n",
      "Epoch 5: loss: 0.041210, validation accuracy: 94.81%\n",
      "Epoch 6: loss: 0.037501, validation accuracy: 95.05%\n",
      "Epoch 7: loss: 0.033919, validation accuracy: 95.26%\n",
      "Epoch 8: loss: 0.030236, validation accuracy: 95.42%\n",
      "Epoch 9: loss: 0.028123, validation accuracy: 95.53%\n",
      "Epoch 10: loss: 0.025412, validation accuracy: 95.66%\n",
      "Epoch 11: loss: 0.023834, validation accuracy: 95.80%\n",
      "Epoch 12: loss: 0.022668, validation accuracy: 95.90%\n",
      "Epoch 13: loss: 0.021826, validation accuracy: 95.99%\n",
      "Epoch 14: loss: 0.020174, validation accuracy: 96.10%\n",
      "Epoch 15: loss: 0.019718, validation accuracy: 96.16%\n",
      "Epoch 16: loss: 0.018719, validation accuracy: 96.21%\n",
      "Epoch 17: loss: 0.017867, validation accuracy: 96.23%\n",
      "Epoch 18: loss: 0.017146, validation accuracy: 96.28%\n",
      "Epoch 19: loss: 0.017011, validation accuracy: 96.31%\n",
      "Epoch 20: loss: 0.016633, validation accuracy: 96.30%\n",
      "Epoch 21: loss: 0.016149, validation accuracy: 96.34%\n",
      "Epoch 22: loss: 0.016006, validation accuracy: 96.38%\n",
      "Epoch 23: loss: 0.016206, validation accuracy: 96.41%\n",
      "Epoch 24: loss: 0.016153, validation accuracy: 96.44%\n",
      "Epoch 25: loss: 0.015976, validation accuracy: 96.45%\n",
      "Epoch 26: loss: 0.015706, validation accuracy: 96.47%\n",
      "Epoch 27: loss: 0.015513, validation accuracy: 96.50%\n",
      "Epoch 28: loss: 0.015596, validation accuracy: 96.53%\n",
      "Epoch 29: loss: 0.015546, validation accuracy: 96.54%\n",
      "Epoch 30: loss: 0.016069, validation accuracy: 96.56%\n",
      "Epoch 31: loss: 0.015996, validation accuracy: 96.57%\n",
      "Epoch 32: loss: 0.016070, validation accuracy: 96.59%\n",
      "Epoch 33: loss: 0.016164, validation accuracy: 96.60%\n",
      "Epoch 34: loss: 0.015720, validation accuracy: 96.62%\n",
      "Epoch 35: loss: 0.015622, validation accuracy: 96.62%\n",
      "Epoch 36: loss: 0.015326, validation accuracy: 96.63%\n",
      "Epoch 37: loss: 0.015383, validation accuracy: 96.64%\n",
      "Epoch 38: loss: 0.015092, validation accuracy: 96.66%\n",
      "Epoch 39: loss: 0.015146, validation accuracy: 96.68%\n",
      "Epoch 40: loss: 0.015094, validation accuracy: 96.69%\n",
      "Epoch 41: loss: 0.015510, validation accuracy: 96.71%\n",
      "Epoch 42: loss: 0.015112, validation accuracy: 96.73%\n",
      "Epoch 43: loss: 0.014800, validation accuracy: 96.72%\n",
      "Epoch 44: loss: 0.014620, validation accuracy: 96.75%\n",
      "Epoch 45: loss: 0.014204, validation accuracy: 96.76%\n",
      "Epoch 46: loss: 0.014083, validation accuracy: 96.75%\n",
      "Epoch 47: loss: 0.014510, validation accuracy: 96.78%\n",
      "Epoch 48: loss: 0.014371, validation accuracy: 96.78%\n",
      "Epoch 49: loss: 0.014136, validation accuracy: 96.81%\n",
      "Epoch 50: loss: 0.013661, validation accuracy: 96.84%\n",
      "Epoch 51: loss: 0.014092, validation accuracy: 96.84%\n",
      "Epoch 52: loss: 0.013587, validation accuracy: 96.86%\n",
      "Epoch 53: loss: 0.013606, validation accuracy: 96.87%\n",
      "Epoch 54: loss: 0.014160, validation accuracy: 96.87%\n",
      "Epoch 55: loss: 0.013467, validation accuracy: 96.88%\n",
      "Epoch 56: loss: 0.013730, validation accuracy: 96.90%\n",
      "Epoch 57: loss: 0.013652, validation accuracy: 96.91%\n",
      "Epoch 58: loss: 0.013714, validation accuracy: 96.92%\n",
      "Epoch 59: loss: 0.013200, validation accuracy: 96.92%\n",
      "Epoch 60: loss: 0.013620, validation accuracy: 96.91%\n",
      "Epoch 61: loss: 0.013446, validation accuracy: 96.92%\n",
      "Epoch 62: loss: 0.013198, validation accuracy: 96.95%\n",
      "Epoch 63: loss: 0.013216, validation accuracy: 96.95%\n",
      "Epoch 64: loss: 0.012849, validation accuracy: 96.96%\n",
      "Epoch 65: loss: 0.012629, validation accuracy: 96.93%\n",
      "Epoch 66: loss: 0.012819, validation accuracy: 96.96%\n",
      "Epoch 67: loss: 0.012285, validation accuracy: 96.97%\n",
      "Epoch 68: loss: 0.012832, validation accuracy: 96.97%\n",
      "Epoch 69: loss: 0.012699, validation accuracy: 96.96%\n",
      "Epoch 70: loss: 0.012685, validation accuracy: 96.96%\n",
      "Epoch 71: loss: 0.012680, validation accuracy: 96.97%\n",
      "Epoch 72: loss: 0.012954, validation accuracy: 96.96%\n",
      "Epoch 73: loss: 0.012694, validation accuracy: 96.98%\n",
      "Epoch 74: loss: 0.012737, validation accuracy: 96.95%\n",
      "Epoch 75: loss: 0.013185, validation accuracy: 96.95%\n",
      "Epoch 76: loss: 0.012965, validation accuracy: 96.95%\n",
      "Epoch 77: loss: 0.012363, validation accuracy: 96.97%\n",
      "Epoch 78: loss: 0.012896, validation accuracy: 96.97%\n",
      "Epoch 79: loss: 0.012608, validation accuracy: 96.95%\n",
      "Epoch 80: loss: 0.012768, validation accuracy: 96.96%\n",
      "Epoch 81: loss: 0.012620, validation accuracy: 96.97%\n",
      "Epoch 82: loss: 0.012310, validation accuracy: 96.98%\n",
      "Epoch 83: loss: 0.012550, validation accuracy: 96.98%\n",
      "Epoch 84: loss: 0.013188, validation accuracy: 97.00%\n",
      "Epoch 85: loss: 0.012817, validation accuracy: 96.99%\n",
      "Epoch 86: loss: 0.012909, validation accuracy: 96.98%\n",
      "Epoch 87: loss: 0.013157, validation accuracy: 97.00%\n",
      "Epoch 88: loss: 0.013192, validation accuracy: 97.00%\n",
      "Epoch 89: loss: 0.013204, validation accuracy: 96.98%\n",
      "Epoch 90: loss: 0.013060, validation accuracy: 97.00%\n",
      "Epoch 91: loss: 0.013450, validation accuracy: 97.01%\n",
      "Epoch 92: loss: 0.013360, validation accuracy: 97.00%\n",
      "Epoch 93: loss: 0.013858, validation accuracy: 97.02%\n",
      "Epoch 94: loss: 0.013523, validation accuracy: 97.02%\n",
      "Epoch 95: loss: 0.013973, validation accuracy: 97.03%\n",
      "Epoch 96: loss: 0.013533, validation accuracy: 97.04%\n",
      "Epoch 97: loss: 0.013748, validation accuracy: 97.03%\n",
      "Epoch 98: loss: 0.013684, validation accuracy: 97.04%\n",
      "Epoch 99: loss: 0.013766, validation accuracy: 97.03%\n",
      "Epoch 100: loss: 0.013763, validation accuracy: 97.04%\n",
      "Epoch 101: loss: 0.014440, validation accuracy: 97.04%\n",
      "Epoch 102: loss: 0.013667, validation accuracy: 97.05%\n",
      "Epoch 103: loss: 0.014237, validation accuracy: 97.04%\n",
      "Epoch 104: loss: 0.014217, validation accuracy: 97.05%\n",
      "Epoch 105: loss: 0.014091, validation accuracy: 97.03%\n",
      "Epoch 106: loss: 0.014108, validation accuracy: 97.05%\n",
      "Epoch 107: loss: 0.014663, validation accuracy: 97.06%\n",
      "Epoch 108: loss: 0.013957, validation accuracy: 97.06%\n",
      "Epoch 109: loss: 0.014177, validation accuracy: 97.05%\n",
      "Epoch 110: loss: 0.014045, validation accuracy: 97.05%\n",
      "Epoch 111: loss: 0.014419, validation accuracy: 97.05%\n",
      "Epoch 112: loss: 0.014082, validation accuracy: 97.07%\n",
      "Epoch 113: loss: 0.013962, validation accuracy: 97.06%\n",
      "Epoch 114: loss: 0.014183, validation accuracy: 97.07%\n",
      "Epoch 115: loss: 0.013981, validation accuracy: 97.10%\n",
      "Epoch 116: loss: 0.013930, validation accuracy: 97.08%\n",
      "Epoch 117: loss: 0.014102, validation accuracy: 97.10%\n",
      "Epoch 118: loss: 0.013533, validation accuracy: 97.10%\n",
      "Epoch 119: loss: 0.012818, validation accuracy: 97.06%\n",
      "Epoch 120: loss: 0.012803, validation accuracy: 97.07%\n",
      "Epoch 121: loss: 0.012858, validation accuracy: 97.07%\n",
      "Epoch 122: loss: 0.012749, validation accuracy: 97.08%\n",
      "Epoch 123: loss: 0.012744, validation accuracy: 97.06%\n",
      "Epoch 124: loss: 0.012758, validation accuracy: 97.06%\n",
      "Epoch 125: loss: 0.012770, validation accuracy: 97.08%\n",
      "Epoch 126: loss: 0.012745, validation accuracy: 97.06%\n",
      "Epoch 127: loss: 0.012543, validation accuracy: 97.10%\n",
      "Epoch 128: loss: 0.013068, validation accuracy: 97.11%\n",
      "Epoch 129: loss: 0.012595, validation accuracy: 97.09%\n",
      "Epoch 130: loss: 0.012617, validation accuracy: 97.12%\n",
      "Epoch 131: loss: 0.012400, validation accuracy: 97.12%\n",
      "Epoch 132: loss: 0.012874, validation accuracy: 97.12%\n",
      "Epoch 133: loss: 0.012583, validation accuracy: 97.12%\n",
      "Epoch 134: loss: 0.012496, validation accuracy: 97.11%\n",
      "Epoch 135: loss: 0.013194, validation accuracy: 97.14%\n",
      "Epoch 136: loss: 0.012817, validation accuracy: 97.12%\n",
      "Epoch 137: loss: 0.012748, validation accuracy: 97.12%\n",
      "Epoch 138: loss: 0.012690, validation accuracy: 97.15%\n",
      "Epoch 139: loss: 0.012550, validation accuracy: 97.13%\n",
      "Epoch 140: loss: 0.013006, validation accuracy: 97.14%\n",
      "Epoch 141: loss: 0.012664, validation accuracy: 97.14%\n",
      "Epoch 142: loss: 0.012900, validation accuracy: 97.15%\n",
      "Epoch 143: loss: 0.013179, validation accuracy: 97.14%\n",
      "Epoch 144: loss: 0.012707, validation accuracy: 97.13%\n",
      "Epoch 145: loss: 0.012776, validation accuracy: 97.16%\n",
      "Epoch 146: loss: 0.012532, validation accuracy: 97.15%\n",
      "Epoch 147: loss: 0.012811, validation accuracy: 97.16%\n",
      "Epoch 148: loss: 0.012684, validation accuracy: 97.19%\n",
      "Epoch 149: loss: 0.012961, validation accuracy: 97.18%\n",
      "Epoch 150: loss: 0.012732, validation accuracy: 97.20%\n",
      "Epoch 151: loss: 0.012804, validation accuracy: 97.17%\n",
      "Epoch 152: loss: 0.012553, validation accuracy: 97.19%\n",
      "Epoch 153: loss: 0.012720, validation accuracy: 97.18%\n",
      "Epoch 154: loss: 0.013014, validation accuracy: 97.20%\n",
      "Epoch 155: loss: 0.012613, validation accuracy: 97.18%\n",
      "Epoch 156: loss: 0.012706, validation accuracy: 97.18%\n",
      "Epoch 157: loss: 0.012690, validation accuracy: 97.19%\n",
      "Epoch 158: loss: 0.012745, validation accuracy: 97.19%\n",
      "Epoch 159: loss: 0.012948, validation accuracy: 97.19%\n",
      "Epoch 160: loss: 0.012868, validation accuracy: 97.18%\n",
      "Epoch 161: loss: 0.012824, validation accuracy: 97.19%\n",
      "Epoch 162: loss: 0.012439, validation accuracy: 97.20%\n",
      "Epoch 163: loss: 0.012465, validation accuracy: 97.19%\n",
      "Epoch 164: loss: 0.013044, validation accuracy: 97.23%\n",
      "Epoch 165: loss: 0.012889, validation accuracy: 97.19%\n",
      "Epoch 166: loss: 0.012154, validation accuracy: 97.20%\n",
      "Epoch 167: loss: 0.012632, validation accuracy: 97.19%\n",
      "Epoch 168: loss: 0.012708, validation accuracy: 97.22%\n",
      "Epoch 169: loss: 0.012513, validation accuracy: 97.17%\n",
      "Epoch 170: loss: 0.012402, validation accuracy: 97.21%\n",
      "Epoch 171: loss: 0.012602, validation accuracy: 97.19%\n",
      "Epoch 172: loss: 0.012440, validation accuracy: 97.18%\n",
      "Epoch 173: loss: 0.012815, validation accuracy: 97.19%\n",
      "Epoch 174: loss: 0.012507, validation accuracy: 97.19%\n",
      "Epoch 175: loss: 0.012918, validation accuracy: 97.22%\n",
      "Stopping training: no improvement for 10 epochs\n",
      "Test set performance: 96.20%\n"
     ]
    }
   ],
   "source": [
    "best_validation_accuracy = 0\n",
    "best_epoch = -1\n",
    "\n",
    "print(\"Training the model...\")\n",
    "print(\"Shape of the training set: \", train_validation_set.images[train_indices].shape)\n",
    "print(\"Shape of the labels: \", train_validation_set.labels[train_indices].shape)\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Train the model using the training set\n",
    "    train_set = train_validation_set.images[train_indices]\n",
    "    target = train_validation_set.labels[train_indices]\n",
    "\n",
    "    count = 0\n",
    "    total_ok = 0\n",
    "    for i in range(0, len(train_set), batch_size):\n",
    "        # Compute end index of the batch\n",
    "        end = i + batch_size\n",
    "        if end > len(train_set):\n",
    "            end = len(train_set)\n",
    "            \n",
    "        assert end <= len(train_set), \"Batch size is too large\"\n",
    "\n",
    "        # train data: 64 images (28x28 pixels) --> 64x28x28\n",
    "        # reshape the data to 64x784 (inline image)\n",
    "\n",
    "        # Split the training samples + targets into batches\n",
    "        train_batch = train_set[i: end].reshape(-1, 28*28)\n",
    "        target_batch = target[i: end]\n",
    "\n",
    "        # Train the model using the current batch\n",
    "        current_loss_value = train_one_step(model, loss, learning_rate, train_batch, target_batch)\n",
    "\n",
    "    # Now, we have to compute the validation accuracy (using the validation set)\n",
    "    validation_accuracy = validate()\n",
    "    print(\"Epoch %d: loss: %f, validation accuracy: %.2f%%\" % (epoch, current_loss_value, validation_accuracy))\n",
    "\n",
    "    # Early stopping: stop training if the model has not improved in the last M epochs\n",
    "    M = 10\n",
    "\n",
    "    # Check if the new model is better\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        if epoch - best_epoch > M:\n",
    "            print(\"Stopping training: no improvement for %d epochs\" % M)\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"Test set performance: %.2f%%\" % test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick a random image from the test set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick a random image rom the test set\n",
    "index = np.random.randint(len(test_set))\n",
    "\n",
    "# Get the image and the label\n",
    "image = test_set.images[index]\n",
    "label = test_set.labels[index]\n",
    "\n",
    "# Forward pass\n",
    "pred = model.forward(image.reshape(1, -1))\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = np.argmax(pred)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image.reshape(28, 28), cmap=\"gray\")\n",
    "\n",
    "# Print the predicted class and the true class\n",
    "print(\"Model prediction:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
