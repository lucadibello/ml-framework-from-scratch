{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Handwritten Digit Recognition\n",
    "\n",
    "This example shows how to use the machine learning framework to train a model for handwritten digit recognition. For this task, we will use the MNIST dataset, which is a dataset of 28x28 grayscale images of handwritten digits (0-9). The dataset contains 42,000 training images and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import required framework layers\n",
    "from framework.layers import Sequential, Linear\n",
    "from framework.activations import ReLU\n",
    "from framework.loss import SoftmaxCrossEntropy\n",
    "from framework.network import train_one_step\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod_0: <framework.layers.linear.Linear object at 0x10c5cad80>\n",
      "mod_1: <framework.activations.relu.ReLU object at 0x10c5c8800>\n",
      "mod_2: <framework.layers.linear.Linear object at 0x10c5ca9f0>\n"
     ]
    }
   ],
   "source": [
    "# Create simple network\n",
    "model = Sequential([\n",
    "    Linear(28*28, 20), # 28*28 input features (flat images), 20 output features\n",
    "    ReLU(), # Activation function\n",
    "    Linear(20, 10), # 20 input features, 10 output features\n",
    "])\n",
    "\n",
    "# Define loss function\n",
    "loss = SoftmaxCrossEntropy()\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# Learning rate: step size for the optimizer\n",
    "learning_rate = 0.1\n",
    "# Batch size: number of samples per batch\n",
    "batch_size = 64\n",
    "# Number of epochs: number of times the dataset will be passed through the network\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST:\n",
      "   Train set size: 42000\n",
      "   Validation set size: 18000\n",
      "   Test set size 10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MNIST:\n",
    "    URL = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "\n",
    "    def __init__(self, set, cache=\"./cache\"):\n",
    "        os.makedirs(cache, exist_ok=True)\n",
    "\n",
    "        path = os.path.join(cache, \"mnist.npz\")\n",
    "        if not os.path.isfile(path):\n",
    "            request.urlretrieve(self.URL, path)\n",
    "\n",
    "        data = np.load(path)\n",
    "\n",
    "        assert set in [\"train\", \"test\"], \"set must be either train or test\"\n",
    "\n",
    "        self.images = data[\"x_\" + set].astype(float) / 255.0\n",
    "        self.labels = data[\"y_\" + set]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.images.shape[0]\n",
    "\n",
    "\n",
    "train_validation_set = MNIST(\"train\")\n",
    "test_set = MNIST(\"test\")\n",
    "\n",
    "n_train = int(0.7 * len(train_validation_set))\n",
    "print(\"MNIST:\")\n",
    "print(\"   Train set size:\", n_train)\n",
    "print(\"   Validation set size:\", len(train_validation_set) - n_train)\n",
    "print(\"   Test set size\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(train_validation_set))\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_indices = indices[:n_train]\n",
    "validation_indices = indices[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliy functions for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def verify(images: np.ndarray, targets: np.ndarray) -> Tuple[int, int]:\n",
    "    # Forward pass\n",
    "    pred = model.forward(images)\n",
    "\n",
    "    # Comupute number of correct predictions\n",
    "    # NOTE: The maximum value in the probability vector is the class chosen by the model\n",
    "    #       argmax --> return the index of the maximum value == predicted class by the model\n",
    "    num_ok = np.sum(np.argmax(pred, axis=1) == targets)\n",
    "    # Total number of images\n",
    "    total_num = targets.shape[0]\n",
    "    \n",
    "    return num_ok, total_num\n",
    "\n",
    "\n",
    "def test() -> float:\n",
    "    accumulated_ok = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # use test set\n",
    "    for i in range(0, len(test_set), batch_size):\n",
    "        images = test_set.images[i:i + batch_size].reshape(-1, 28*28)\n",
    "        labels = test_set.labels[i:i + batch_size]\n",
    "\n",
    "        # Verify the data\n",
    "        num_ok, total_num = verify(images, labels)\n",
    "        accumulated_ok += num_ok\n",
    "        count += total_num\n",
    "\n",
    "    return accumulated_ok / count * 100.0\n",
    "\n",
    "\n",
    "def validate() -> float:\n",
    "    accumulated_ok = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # use validation set\n",
    "    for i in range(0, len(train_validation_set), batch_size):\n",
    "        images = train_validation_set.images[i:i + batch_size].reshape(-1, 28*28)\n",
    "        labels = train_validation_set.labels[i:i + batch_size]\n",
    "\n",
    "        # Verify the data\n",
    "        num_ok, total_num = verify(images, labels)\n",
    "        accumulated_ok += num_ok\n",
    "        count += total_num\n",
    "\n",
    "    return accumulated_ok/count * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with early stopping\n",
    "\n",
    "Rather than training the model for a fixed number of epochs, we will use early stopping to stop training when the validation loss stops decreasing. This helps prevent overfitting and can improve the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Shape of the training set:  (42000, 28, 28)\n",
      "Shape of the labels:  (42000,)\n",
      "Epoch 0: loss: 0.178793, validation accuracy: 91.27%\n",
      "Epoch 1: loss: 0.089356, validation accuracy: 92.77%\n",
      "Epoch 2: loss: 0.063351, validation accuracy: 93.55%\n",
      "Epoch 3: loss: 0.052029, validation accuracy: 94.08%\n",
      "Epoch 4: loss: 0.045468, validation accuracy: 94.47%\n",
      "Epoch 5: loss: 0.041210, validation accuracy: 94.81%\n",
      "Epoch 6: loss: 0.037501, validation accuracy: 95.05%\n",
      "Epoch 7: loss: 0.033919, validation accuracy: 95.26%\n",
      "Epoch 8: loss: 0.030236, validation accuracy: 95.42%\n",
      "Epoch 9: loss: 0.028123, validation accuracy: 95.53%\n",
      "Epoch 10: loss: 0.025412, validation accuracy: 95.66%\n",
      "Epoch 11: loss: 0.023834, validation accuracy: 95.80%\n",
      "Epoch 12: loss: 0.022668, validation accuracy: 95.90%\n",
      "Epoch 13: loss: 0.021826, validation accuracy: 95.99%\n",
      "Epoch 14: loss: 0.020174, validation accuracy: 96.10%\n",
      "Epoch 15: loss: 0.019718, validation accuracy: 96.16%\n",
      "Epoch 16: loss: 0.018719, validation accuracy: 96.21%\n",
      "Epoch 17: loss: 0.017867, validation accuracy: 96.23%\n",
      "Epoch 18: loss: 0.017146, validation accuracy: 96.28%\n",
      "Epoch 19: loss: 0.017011, validation accuracy: 96.31%\n",
      "Epoch 20: loss: 0.016633, validation accuracy: 96.30%\n",
      "Epoch 21: loss: 0.016149, validation accuracy: 96.34%\n",
      "Epoch 22: loss: 0.016006, validation accuracy: 96.38%\n",
      "Epoch 23: loss: 0.016206, validation accuracy: 96.41%\n",
      "Epoch 24: loss: 0.016153, validation accuracy: 96.44%\n",
      "Epoch 25: loss: 0.015976, validation accuracy: 96.45%\n",
      "Epoch 26: loss: 0.015706, validation accuracy: 96.47%\n",
      "Epoch 27: loss: 0.015513, validation accuracy: 96.50%\n",
      "Epoch 28: loss: 0.015596, validation accuracy: 96.53%\n",
      "Epoch 29: loss: 0.015546, validation accuracy: 96.54%\n",
      "Epoch 30: loss: 0.016069, validation accuracy: 96.56%\n",
      "Epoch 31: loss: 0.015996, validation accuracy: 96.57%\n",
      "Epoch 32: loss: 0.016070, validation accuracy: 96.59%\n",
      "Epoch 33: loss: 0.016164, validation accuracy: 96.60%\n",
      "Epoch 34: loss: 0.015720, validation accuracy: 96.62%\n",
      "Epoch 35: loss: 0.015622, validation accuracy: 96.62%\n",
      "Epoch 36: loss: 0.015326, validation accuracy: 96.63%\n",
      "Epoch 37: loss: 0.015383, validation accuracy: 96.64%\n",
      "Epoch 38: loss: 0.015092, validation accuracy: 96.66%\n",
      "Epoch 39: loss: 0.015146, validation accuracy: 96.68%\n",
      "Epoch 40: loss: 0.015094, validation accuracy: 96.69%\n",
      "Epoch 41: loss: 0.015510, validation accuracy: 96.71%\n",
      "Epoch 42: loss: 0.015112, validation accuracy: 96.73%\n",
      "Epoch 43: loss: 0.014800, validation accuracy: 96.72%\n",
      "Epoch 44: loss: 0.014620, validation accuracy: 96.75%\n",
      "Epoch 45: loss: 0.014204, validation accuracy: 96.76%\n",
      "Epoch 46: loss: 0.014083, validation accuracy: 96.75%\n",
      "Epoch 47: loss: 0.014510, validation accuracy: 96.78%\n",
      "Epoch 48: loss: 0.014371, validation accuracy: 96.78%\n",
      "Epoch 49: loss: 0.014136, validation accuracy: 96.81%\n",
      "Epoch 50: loss: 0.013661, validation accuracy: 96.84%\n",
      "Epoch 51: loss: 0.014092, validation accuracy: 96.84%\n",
      "Epoch 52: loss: 0.013587, validation accuracy: 96.86%\n",
      "Epoch 53: loss: 0.013606, validation accuracy: 96.87%\n",
      "Epoch 54: loss: 0.014160, validation accuracy: 96.87%\n",
      "Epoch 55: loss: 0.013467, validation accuracy: 96.88%\n",
      "Epoch 56: loss: 0.013730, validation accuracy: 96.90%\n",
      "Epoch 57: loss: 0.013652, validation accuracy: 96.91%\n",
      "Epoch 58: loss: 0.013714, validation accuracy: 96.92%\n",
      "Epoch 59: loss: 0.013200, validation accuracy: 96.92%\n",
      "Epoch 60: loss: 0.013620, validation accuracy: 96.91%\n",
      "Epoch 61: loss: 0.013446, validation accuracy: 96.92%\n",
      "Epoch 62: loss: 0.013198, validation accuracy: 96.95%\n",
      "Epoch 63: loss: 0.013216, validation accuracy: 96.95%\n",
      "Epoch 64: loss: 0.012849, validation accuracy: 96.96%\n",
      "Epoch 65: loss: 0.012629, validation accuracy: 96.93%\n",
      "Epoch 66: loss: 0.012819, validation accuracy: 96.96%\n",
      "Epoch 67: loss: 0.012285, validation accuracy: 96.97%\n",
      "Epoch 68: loss: 0.012832, validation accuracy: 96.97%\n",
      "Epoch 69: loss: 0.012699, validation accuracy: 96.96%\n",
      "Epoch 70: loss: 0.012685, validation accuracy: 96.96%\n",
      "Epoch 71: loss: 0.012680, validation accuracy: 96.97%\n",
      "Epoch 72: loss: 0.012954, validation accuracy: 96.96%\n",
      "Epoch 73: loss: 0.012694, validation accuracy: 96.98%\n",
      "Epoch 74: loss: 0.012737, validation accuracy: 96.95%\n",
      "Epoch 75: loss: 0.013185, validation accuracy: 96.95%\n",
      "Epoch 76: loss: 0.012965, validation accuracy: 96.95%\n",
      "Epoch 77: loss: 0.012363, validation accuracy: 96.97%\n",
      "Epoch 78: loss: 0.012896, validation accuracy: 96.97%\n",
      "Epoch 79: loss: 0.012608, validation accuracy: 96.95%\n",
      "Epoch 80: loss: 0.012768, validation accuracy: 96.96%\n",
      "Epoch 81: loss: 0.012620, validation accuracy: 96.97%\n",
      "Epoch 82: loss: 0.012310, validation accuracy: 96.98%\n",
      "Epoch 83: loss: 0.012550, validation accuracy: 96.98%\n",
      "Epoch 84: loss: 0.013188, validation accuracy: 97.00%\n",
      "Epoch 85: loss: 0.012817, validation accuracy: 96.99%\n",
      "Epoch 86: loss: 0.012909, validation accuracy: 96.98%\n",
      "Epoch 87: loss: 0.013157, validation accuracy: 97.00%\n",
      "Epoch 88: loss: 0.013192, validation accuracy: 97.00%\n",
      "Epoch 89: loss: 0.013204, validation accuracy: 96.98%\n",
      "Epoch 90: loss: 0.013060, validation accuracy: 97.00%\n",
      "Epoch 91: loss: 0.013450, validation accuracy: 97.01%\n",
      "Epoch 92: loss: 0.013360, validation accuracy: 97.00%\n",
      "Epoch 93: loss: 0.013858, validation accuracy: 97.02%\n",
      "Epoch 94: loss: 0.013523, validation accuracy: 97.02%\n",
      "Epoch 95: loss: 0.013973, validation accuracy: 97.03%\n",
      "Epoch 96: loss: 0.013533, validation accuracy: 97.04%\n",
      "Epoch 97: loss: 0.013748, validation accuracy: 97.03%\n",
      "Epoch 98: loss: 0.013684, validation accuracy: 97.04%\n",
      "Epoch 99: loss: 0.013766, validation accuracy: 97.03%\n",
      "Epoch 100: loss: 0.013763, validation accuracy: 97.04%\n",
      "Epoch 101: loss: 0.014440, validation accuracy: 97.04%\n",
      "Epoch 102: loss: 0.013667, validation accuracy: 97.05%\n",
      "Epoch 103: loss: 0.014237, validation accuracy: 97.04%\n",
      "Epoch 104: loss: 0.014217, validation accuracy: 97.05%\n",
      "Epoch 105: loss: 0.014091, validation accuracy: 97.03%\n",
      "Epoch 106: loss: 0.014108, validation accuracy: 97.05%\n",
      "Epoch 107: loss: 0.014663, validation accuracy: 97.06%\n",
      "Epoch 108: loss: 0.013957, validation accuracy: 97.06%\n",
      "Epoch 109: loss: 0.014177, validation accuracy: 97.05%\n",
      "Epoch 110: loss: 0.014045, validation accuracy: 97.05%\n",
      "Epoch 111: loss: 0.014419, validation accuracy: 97.05%\n",
      "Epoch 112: loss: 0.014082, validation accuracy: 97.07%\n",
      "Epoch 113: loss: 0.013962, validation accuracy: 97.06%\n",
      "Epoch 114: loss: 0.014183, validation accuracy: 97.07%\n",
      "Epoch 115: loss: 0.013981, validation accuracy: 97.10%\n",
      "Epoch 116: loss: 0.013930, validation accuracy: 97.08%\n",
      "Epoch 117: loss: 0.014102, validation accuracy: 97.10%\n",
      "Epoch 118: loss: 0.013533, validation accuracy: 97.10%\n",
      "Epoch 119: loss: 0.012818, validation accuracy: 97.06%\n",
      "Epoch 120: loss: 0.012803, validation accuracy: 97.07%\n",
      "Epoch 121: loss: 0.012858, validation accuracy: 97.07%\n",
      "Epoch 122: loss: 0.012749, validation accuracy: 97.08%\n",
      "Epoch 123: loss: 0.012744, validation accuracy: 97.06%\n",
      "Epoch 124: loss: 0.012758, validation accuracy: 97.06%\n",
      "Epoch 125: loss: 0.012770, validation accuracy: 97.08%\n",
      "Epoch 126: loss: 0.012745, validation accuracy: 97.06%\n",
      "Epoch 127: loss: 0.012543, validation accuracy: 97.10%\n",
      "Epoch 128: loss: 0.013068, validation accuracy: 97.11%\n",
      "Epoch 129: loss: 0.012595, validation accuracy: 97.09%\n",
      "Epoch 130: loss: 0.012617, validation accuracy: 97.12%\n",
      "Epoch 131: loss: 0.012400, validation accuracy: 97.12%\n",
      "Epoch 132: loss: 0.012874, validation accuracy: 97.12%\n",
      "Epoch 133: loss: 0.012583, validation accuracy: 97.12%\n",
      "Epoch 134: loss: 0.012496, validation accuracy: 97.11%\n",
      "Epoch 135: loss: 0.013194, validation accuracy: 97.14%\n",
      "Epoch 136: loss: 0.012817, validation accuracy: 97.12%\n",
      "Epoch 137: loss: 0.012748, validation accuracy: 97.12%\n",
      "Epoch 138: loss: 0.012690, validation accuracy: 97.15%\n",
      "Epoch 139: loss: 0.012550, validation accuracy: 97.13%\n",
      "Epoch 140: loss: 0.013006, validation accuracy: 97.14%\n",
      "Epoch 141: loss: 0.012664, validation accuracy: 97.14%\n",
      "Epoch 142: loss: 0.012900, validation accuracy: 97.15%\n",
      "Epoch 143: loss: 0.013179, validation accuracy: 97.14%\n",
      "Epoch 144: loss: 0.012707, validation accuracy: 97.13%\n",
      "Epoch 145: loss: 0.012776, validation accuracy: 97.16%\n",
      "Epoch 146: loss: 0.012532, validation accuracy: 97.15%\n",
      "Epoch 147: loss: 0.012811, validation accuracy: 97.16%\n",
      "Epoch 148: loss: 0.012684, validation accuracy: 97.19%\n",
      "Epoch 149: loss: 0.012961, validation accuracy: 97.18%\n",
      "Epoch 150: loss: 0.012732, validation accuracy: 97.20%\n",
      "Epoch 151: loss: 0.012804, validation accuracy: 97.17%\n",
      "Epoch 152: loss: 0.012553, validation accuracy: 97.19%\n",
      "Epoch 153: loss: 0.012720, validation accuracy: 97.18%\n",
      "Epoch 154: loss: 0.013014, validation accuracy: 97.20%\n",
      "Epoch 155: loss: 0.012613, validation accuracy: 97.18%\n",
      "Epoch 156: loss: 0.012706, validation accuracy: 97.18%\n",
      "Epoch 157: loss: 0.012690, validation accuracy: 97.19%\n",
      "Epoch 158: loss: 0.012745, validation accuracy: 97.19%\n",
      "Epoch 159: loss: 0.012948, validation accuracy: 97.19%\n",
      "Epoch 160: loss: 0.012868, validation accuracy: 97.18%\n",
      "Epoch 161: loss: 0.012824, validation accuracy: 97.19%\n",
      "Epoch 162: loss: 0.012439, validation accuracy: 97.20%\n",
      "Epoch 163: loss: 0.012465, validation accuracy: 97.19%\n",
      "Epoch 164: loss: 0.013044, validation accuracy: 97.23%\n",
      "Epoch 165: loss: 0.012889, validation accuracy: 97.19%\n",
      "Epoch 166: loss: 0.012154, validation accuracy: 97.20%\n",
      "Epoch 167: loss: 0.012632, validation accuracy: 97.19%\n",
      "Epoch 168: loss: 0.012708, validation accuracy: 97.22%\n",
      "Epoch 169: loss: 0.012513, validation accuracy: 97.17%\n",
      "Epoch 170: loss: 0.012402, validation accuracy: 97.21%\n",
      "Epoch 171: loss: 0.012602, validation accuracy: 97.19%\n",
      "Epoch 172: loss: 0.012440, validation accuracy: 97.18%\n",
      "Epoch 173: loss: 0.012815, validation accuracy: 97.19%\n",
      "Epoch 174: loss: 0.012507, validation accuracy: 97.19%\n",
      "Epoch 175: loss: 0.012918, validation accuracy: 97.22%\n",
      "Stopping training: no improvement for 10 epochs\n",
      "Test set performance: 96.20%\n"
     ]
    }
   ],
   "source": [
    "best_validation_accuracy = 0\n",
    "best_epoch = -1\n",
    "\n",
    "print(\"Training the model...\")\n",
    "print(\"Shape of the training set: \", train_validation_set.images[train_indices].shape)\n",
    "print(\"Shape of the labels: \", train_validation_set.labels[train_indices].shape)\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Train the model using the training set\n",
    "    train_set = train_validation_set.images[train_indices]\n",
    "    target = train_validation_set.labels[train_indices]\n",
    "\n",
    "    count = 0\n",
    "    total_ok = 0\n",
    "    for i in range(0, len(train_set), batch_size):\n",
    "        # Compute end index of the batch\n",
    "        end = i + batch_size\n",
    "        if end > len(train_set):\n",
    "            end = len(train_set)\n",
    "            \n",
    "        assert end <= len(train_set), \"Batch size is too large\"\n",
    "\n",
    "        # train data: 64 images (28x28 pixels) --> 64x28x28\n",
    "        # reshape the data to 64x784 (inline image)\n",
    "\n",
    "        # Split the training samples + targets into batches\n",
    "        train_batch = train_set[i: end].reshape(-1, 28*28)\n",
    "        target_batch = target[i: end]\n",
    "\n",
    "        # Train the model using the current batch\n",
    "        current_loss_value = train_one_step(model, loss, learning_rate, train_batch, target_batch)\n",
    "\n",
    "    # Now, we have to compute the validation accuracy (using the validation set)\n",
    "    validation_accuracy = validate()\n",
    "    print(\"Epoch %d: loss: %f, validation accuracy: %.2f%%\" % (epoch, current_loss_value, validation_accuracy))\n",
    "\n",
    "    # Early stopping: stop training if the model has not improved in the last M epochs\n",
    "    M = 10\n",
    "\n",
    "    # Check if the new model is better\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        if epoch - best_epoch > M:\n",
    "            print(\"Stopping training: no improvement for %d epochs\" % M)\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"Test set performance: %.2f%%\" % test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZ4ElEQVR4nO3df2zUdx3H8dcNyvHD6ymB9q5QmrqAU0DiAAuE8WOBhsbVMaaykZiSGdzkR4IdTitZqP5BJ2bN/sCxOA3CHBMTfowEMqiBtkxEgXQBAZFJkVtKrTR4Vwprx/j4B+GyWzvge9zx7rXPR/JN1rvve/fhu+948uWu3/qcc04AABh4wHoBAIC+iwgBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz/a0X8Gk3btxQU1OTAoGAfD6f9XIAAB4559TW1qa8vDw98MDtr3V6XISampqUn59vvQwAwD2KRCIaOXLkbffpcX8dFwgErJcAAEiBu/n9PG0RevXVV1VYWKiBAwdq4sSJOnjw4F3N8VdwANA73M3v52mJ0NatW7Vy5UqtXr1aDQ0NeuSRR1RSUqILFy6k4+UAABnKl467aBcVFenhhx/Whg0b4o99+ctf1vz581VVVXXb2VgspmAwmOolAQDus2g0quzs7Nvuk/Iroc7OTh07dkzFxcUJjxcXF+vQoUNd9u/o6FAsFkvYAAB9Q8ojdOnSJX388cfKzc1NeDw3N1fNzc1d9q+qqlIwGIxvfDIOAPqOtH0w4dNvSDnnun2TqqKiQtFoNL5FIpF0LQkA0MOk/PuEhg0bpn79+nW56mlpaelydSRJfr9ffr8/1csAAGSAlF8JDRgwQBMnTlRNTU3C4zU1NZo2bVqqXw4AkMHScseE8vJyffe739WkSZM0depU/frXv9aFCxf03HPPpePlAAAZKi0RWrhwoVpbW/Xzn/9cFy9e1Lhx47Rnzx4VFBSk4+UAABkqLd8ndC/4PiEA6B1Mvk8IAIC7RYQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGb6Wy8Afctjjz3meeaNN97wPPP5z3/e84wk/etf//I8U19f73nmF7/4heeZM2fOeJ4BejquhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM9zAFPfVokWLPM8Eg0HPM//4xz88z0jSl770Jc8zX/ziFz3PzJkzx/NMMsfu3Xff9TwD3E9cCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZnzOOWe9iE+KxWJJ3bASmWHw4MGeZ5K5Qejp06c9z0jSF77wBc8z3/72tz3PVFdXe565fPmy55lvfvObnmck6ejRo0nNAZ8UjUaVnZ192324EgIAmCFCAAAzKY9QZWWlfD5fwhYKhVL9MgCAXiAtP9Ru7Nix+tOf/hT/ul+/ful4GQBAhktLhPr378/VDwDgjtLyntDZs2eVl5enwsJCPfXUUzp37txn7tvR0aFYLJawAQD6hpRHqKioSJs3b9bevXv1+uuvq7m5WdOmTVNra2u3+1dVVSkYDMa3/Pz8VC8JANBDpTxCJSUlevLJJzV+/HjNmTNHu3fvliRt2rSp2/0rKioUjUbjWyQSSfWSAAA9VFreE/qkIUOGaPz48Tp79my3z/v9fvn9/nQvAwDQA6X9+4Q6Ojp0+vRphcPhdL8UACDDpDxCq1atUl1dnRobG/XXv/5V3/rWtxSLxVRWVpbqlwIAZLiU/3XcBx98oKefflqXLl3S8OHDNWXKFB0+fFgFBQWpfikAQIbjBqaAgWRuLPrWW295nrndt0fczpw5czzP/Oc//0nqtdB7cQNTAECPRoQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamQIbYuHGj55lkf4RKdXW155lVq1Yl9VrovbiBKQCgRyNCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZ7qIN9GIfffRRUnORSMTzzEMPPeR5prOz0/MMMgd30QYA9GhECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJn+1gsAkD7Hjx9Pau5rX/ua55msrCzPM9zAFFwJAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmuIEp0It98MEHSc0lcwNTIBlcCQEAzBAhAIAZzxGqr69XaWmp8vLy5PP5tHPnzoTnnXOqrKxUXl6eBg0apFmzZunkyZOpWi8AoBfxHKH29nZNmDBB69ev7/b5devWqbq6WuvXr9eRI0cUCoU0d+5ctbW13fNiAQC9i+cPJpSUlKikpKTb55xzeuWVV7R69WotWLBAkrRp0ybl5uZqy5YtevbZZ+9ttQCAXiWl7wk1NjaqublZxcXF8cf8fr9mzpypQ4cOdTvT0dGhWCyWsAEA+oaURqi5uVmSlJubm/B4bm5u/LlPq6qqUjAYjG/5+fmpXBIAoAdLy6fjfD5fwtfOuS6P3VJRUaFoNBrfIpFIOpYEAOiBUvrNqqFQSNLNK6JwOBx/vKWlpcvV0S1+v19+vz+VywAAZIiUXgkVFhYqFAqppqYm/lhnZ6fq6uo0bdq0VL4UAKAX8HwldOXKFb3//vvxrxsbG/Xee+9p6NChGjVqlFauXKm1a9dq9OjRGj16tNauXavBgwdr0aJFKV04ACDzeY7Q0aNHNXv27PjX5eXlkqSysjL97ne/0wsvvKBr165p6dKlunz5soqKirRv3z4FAoHUrRoA0Cv4nHPOehGfFIvFFAwGrZcB9Apvv/12UnOlpaWeZ5L5g2Z7e7vnGWSOaDSq7Ozs2+7DveMAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmf7WCwDuxO/3e56ZN29eUq/1zDPPeJ7x+XyeZ0pKSjzPnDp1yvPM2LFjPc9I0t///nfPMx0dHUm9Fvo2roQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADPcwBT31YQJEzzP/OY3v/E8M3HiRM8zyfrnP//peaaxsdHzzPjx4z3PJKuwsNDzzFe+8hXPM8ePH/c8g96FKyEAgBkiBAAw4zlC9fX1Ki0tVV5ennw+n3bu3Jnw/OLFi+Xz+RK2KVOmpGq9AIBexHOE2tvbNWHCBK1fv/4z95k3b54uXrwY3/bs2XNPiwQA9E6eP5hQUlJyx58K6ff7FQqFkl4UAKBvSMt7QrW1tcrJydGYMWO0ZMkStbS0fOa+HR0disViCRsAoG9IeYRKSkr05ptvav/+/Xr55Zd15MgRPfroo5/58+erqqoUDAbjW35+fqqXBADooVL+fUILFy6M//O4ceM0adIkFRQUaPfu3VqwYEGX/SsqKlReXh7/OhaLESIA6CPS/s2q4XBYBQUFOnv2bLfP+/1++f3+dC8DANADpf37hFpbWxWJRBQOh9P9UgCADOP5SujKlSt6//334183Njbqvffe09ChQzV06FBVVlbqySefVDgc1vnz5/XTn/5Uw4YN0xNPPJHShQMAMp/nCB09elSzZ8+Of33r/ZyysjJt2LBBJ06c0ObNm/W///1P4XBYs2fP1tatWxUIBFK3agBAr+BzzjnrRXxSLBZTMBi0Xgbuwle/+lXPM4cPH/Y8M3DgQM8z+/bt8zwjSUuWLPE809zcnNRreXXq1CnPMw8++GAaVtK9y5cve5650/ccdudvf/ub5xnYiEajys7Ovu0+3DsOAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZtL+k1XRe33yR7nfrWTuiP3nP//Z80xpaannGUn66KOPkprzas6cOZ5nRowY4XkmGo16npGkZ555xvPMhg0bPM9s27bN80xRUZHnmaamJs8zuD+4EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHADU/R4FRUVnmfu141Ik5XMDUyTufnrj3/8Y88zkrRjxw7PMw0NDZ5nzp0753nmRz/6keeZH/7wh55ncH9wJQQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOEGprivOjs7Pc/897//TcNKUqekpMTzzPe//33PM7FYzPPM8ePHPc8kq6mpyfPMkSNHPM9873vf8zzz4osvep6RpCtXriQ1h7vHlRAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYbmCJpdXV1nmdWrVrleeY73/mO55lf/vKXnmckafr06Z5n/vjHP3qeGTJkiOeZF154wfNMMv+NkpXMzWnXrVvneeaNN97wPLNixQrPM5JUVVWV1BzuHldCAAAzRAgAYMZThKqqqjR58mQFAgHl5ORo/vz5OnPmTMI+zjlVVlYqLy9PgwYN0qxZs3Ty5MmULhoA0Dt4ilBdXZ2WLVumw4cPq6amRtevX1dxcbHa29vj+6xbt07V1dVav369jhw5olAopLlz56qtrS3liwcAZDZPH0x45513Er7euHGjcnJydOzYMc2YMUPOOb3yyitavXq1FixYIEnatGmTcnNztWXLFj377LOpWzkAIOPd03tC0WhUkjR06FBJUmNjo5qbm1VcXBzfx+/3a+bMmTp06FC3/46Ojg7FYrGEDQDQNyQdIeecysvLNX36dI0bN06S1NzcLEnKzc1N2Dc3Nzf+3KdVVVUpGAzGt/z8/GSXBADIMElHaPny5Tp+/LjeeuutLs/5fL6Er51zXR67paKiQtFoNL5FIpFklwQAyDBJfbPqihUrtGvXLtXX12vkyJHxx0OhkKSbV0ThcDj+eEtLS5ero1v8fr/8fn8yywAAZDhPV0LOOS1fvlzbt2/X/v37VVhYmPB8YWGhQqGQampq4o91dnaqrq5O06ZNS82KAQC9hqcroWXLlmnLli16++23FQgE4u/zBINBDRo0SD6fTytXrtTatWs1evRojR49WmvXrtXgwYO1aNGitPwCAACZy1OENmzYIEmaNWtWwuMbN27U4sWLJd28v9W1a9e0dOlSXb58WUVFRdq3b58CgUBKFgwA6D18zjlnvYhPisViCgaD1stAmuzdu9fzzNy5c9OwktRJ5n+hbdu2eZ659Qc9L65evep5BkiVaDSq7Ozs2+7DveMAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghrto474aMWKE55nXXnvN88w3vvENzzPJWr16teeZqqqqNKwE6Fm4izYAoEcjQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMxwA1MAQFpwA1MAQI9GhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmPEUoaqqKk2ePFmBQEA5OTmaP3++zpw5k7DP4sWL5fP5ErYpU6akdNEAgN7BU4Tq6uq0bNkyHT58WDU1Nbp+/bqKi4vV3t6esN+8efN08eLF+LZnz56ULhoA0Dv097LzO++8k/D1xo0blZOTo2PHjmnGjBnxx/1+v0KhUGpWCADote7pPaFoNCpJGjp0aMLjtbW1ysnJ0ZgxY7RkyRK1tLR85r+jo6NDsVgsYQMA9A0+55xLZtA5p8cff1yXL1/WwYMH449v3bpVn/vc51RQUKDGxka9+OKLun79uo4dOya/39/l31NZWamf/exnyf8KAAA9UjQaVXZ29u13cklaunSpKygocJFI5Lb7NTU1uaysLLdt27Zun//www9dNBqNb5FIxEliY2NjY8vwLRqN3rElnt4TumXFihXatWuX6uvrNXLkyNvuGw6HVVBQoLNnz3b7vN/v7/YKCQDQ+3mKkHNOK1as0I4dO1RbW6vCwsI7zrS2tioSiSgcDie9SABA7+TpgwnLli3T73//e23ZskWBQEDNzc1qbm7WtWvXJElXrlzRqlWr9Je//EXnz59XbW2tSktLNWzYMD3xxBNp+QUAADKYl/eB9Bl/77dx40bnnHNXr151xcXFbvjw4S4rK8uNGjXKlZWVuQsXLtz1a0SjUfO/x2RjY2Nju/ftbt4TSvrTcekSi8UUDAatlwEAuEd38+k47h0HADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADDT4yLknLNeAgAgBe7m9/MeF6G2tjbrJQAAUuBufj/3uR526XHjxg01NTUpEAjI5/MlPBeLxZSfn69IJKLs7GyjFdrjONzEcbiJ43ATx+GmnnAcnHNqa2tTXl6eHnjg9tc6/e/Tmu7aAw88oJEjR952n+zs7D59kt3CcbiJ43ATx+EmjsNN1schGAze1X497q/jAAB9BxECAJjJqAj5/X6tWbNGfr/feimmOA43cRxu4jjcxHG4KdOOQ4/7YAIAoO/IqCshAEDvQoQAAGaIEADADBECAJjJqAi9+uqrKiws1MCBAzVx4kQdPHjQekn3VWVlpXw+X8IWCoWsl5V29fX1Ki0tVV5ennw+n3bu3JnwvHNOlZWVysvL06BBgzRr1iydPHnSZrFpdKfjsHjx4i7nx5QpU2wWmyZVVVWaPHmyAoGAcnJyNH/+fJ05cyZhn75wPtzNcciU8yFjIrR161atXLlSq1evVkNDgx555BGVlJTowoUL1ku7r8aOHauLFy/GtxMnTlgvKe3a29s1YcIErV+/vtvn161bp+rqaq1fv15HjhxRKBTS3Llze919CO90HCRp3rx5CefHnj177uMK06+urk7Lli3T4cOHVVNTo+vXr6u4uFjt7e3xffrC+XA3x0HKkPPBZYivf/3r7rnnnkt47KGHHnI/+clPjFZ0/61Zs8ZNmDDBehmmJLkdO3bEv75x44YLhULupZdeij/24YcfumAw6F577TWDFd4fnz4OzjlXVlbmHn/8cZP1WGlpaXGSXF1dnXOu754Pnz4OzmXO+ZARV0KdnZ06duyYiouLEx4vLi7WoUOHjFZl4+zZs8rLy1NhYaGeeuopnTt3znpJphobG9Xc3Jxwbvj9fs2cObPPnRuSVFtbq5ycHI0ZM0ZLlixRS0uL9ZLSKhqNSpKGDh0qqe+eD58+DrdkwvmQERG6dOmSPv74Y+Xm5iY8npubq+bmZqNV3X9FRUXavHmz9u7dq9dff13Nzc2aNm2aWltbrZdm5tZ//75+bkhSSUmJ3nzzTe3fv18vv/yyjhw5okcffVQdHR3WS0sL55zKy8s1ffp0jRs3TlLfPB+6Ow5S5pwPPe4u2rfz6R/t4Jzr8lhvVlJSEv/n8ePHa+rUqXrwwQe1adMmlZeXG67MXl8/NyRp4cKF8X8eN26cJk2apIKCAu3evVsLFiwwXFl6LF++XMePH9e7777b5bm+dD581nHIlPMhI66Ehg0bpn79+nX5k0xLS0uXP/H0JUOGDNH48eN19uxZ66WYufXpQM6NrsLhsAoKCnrl+bFixQrt2rVLBw4cSPjRL33tfPis49Cdnno+ZESEBgwYoIkTJ6qmpibh8ZqaGk2bNs1oVfY6Ojp0+vRphcNh66WYKSwsVCgUSjg3Ojs7VVdX16fPDUlqbW1VJBLpVeeHc07Lly/X9u3btX//fhUWFiY831fOhzsdh+702PPB8EMRnvzhD39wWVlZ7re//a07deqUW7lypRsyZIg7f/689dLum+eff97V1ta6c+fOucOHD7vHHnvMBQKBXn8M2traXENDg2toaHCSXHV1tWtoaHD//ve/nXPOvfTSSy4YDLrt27e7EydOuKefftqFw2EXi8WMV55atzsObW1t7vnnn3eHDh1yjY2N7sCBA27q1KluxIgRveo4/OAHP3DBYNDV1ta6ixcvxrerV6/G9+kL58OdjkMmnQ8ZEyHnnPvVr37lCgoK3IABA9zDDz+c8HHEvmDhwoUuHA67rKwsl5eX5xYsWOBOnjxpvay0O3DggJPUZSsrK3PO3fxY7po1a1woFHJ+v9/NmDHDnThxwnbRaXC743D16lVXXFzshg8f7rKystyoUaNcWVmZu3DhgvWyU6q7X78kt3Hjxvg+feF8uNNxyKTzgR/lAAAwkxHvCQEAeiciBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwMz/AZ9hAnUzh5y2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick a random image from the test set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick a random image rom the test set\n",
    "index = np.random.randint(len(test_set))\n",
    "\n",
    "# Get the image and the label\n",
    "image = test_set.images[index]\n",
    "label = test_set.labels[index]\n",
    "\n",
    "# Forward pass\n",
    "pred = model.forward(image.reshape(1, -1))\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = np.argmax(pred)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image.reshape(28, 28), cmap=\"gray\")\n",
    "\n",
    "# Print the predicted class and the true class\n",
    "print(\"Model prediction:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
